{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares is Maximum Likelihood\n",
    "\n",
    "We've seen how to learn a linear model that minimizes the sum of squares error on the training dataset. For example, let's say we learn the true parameters of the dataset we have been working with:\n",
    "\n",
    "\\\\[\n",
    "\\theta_0 = 100\\\\\n",
    "\\theta_1 = 5.0\\\\\n",
    "\\hat{y} = \\theta_0 + \\theta_1 x = 100 + 5.0 x\n",
    "\\\\]\n",
    "\n",
    "Now, we know from the training dataset that this relationship isn't always exactly true: practically no datapoints lie exactly on this line. For every training datapoint $x_i$, there is an error $y_i - \\hat{y}_i$. My question is this: how do we account for this error? Why isn't $y_i$ exactly $\\hat{y}_i$?\n",
    "\n",
    "In one of the first readings, we said that we would assume that there was some normally distributed noise that was added to $\\hat{y}_i$. The theory was that there were potentially many unobserved factors which impact $y_i$ in a way that cannot be accounted for by knowing $x_i$. We assume the error is normally distributed because the Central Limit Theorem says that the cumulative effect of many independent unobserved factors is just like a single normally distributed variable.\n",
    "\n",
    "That is, we are saying that:\n",
    "\n",
    "\\\\[\n",
    "y \\sim \\theta_0 + \\theta_1 x + \\mathcal{N}(\\mu = 0, \\sigma = \\sigma')\n",
    "\\\\]\n",
    "\n",
    "Where $\\mathcal{N}(\\mu = 0, \\sigma = \\sigma')$ means some noise that is generated from a normal distribution with mean zero, and some variance $\\sigma'$. The $\\sim$ tilde means \"is distributed as\". This means:\n",
    "\n",
    "\\\\[\n",
    "\\text{Pr}_\\theta[Y = y | X = x]\n",
    "=\n",
    "\\text{Pr}_\\theta[\\mathcal{N}(\\mu = 0, \\sigma = \\sigma') =  y - (\\theta_0 + \\theta_1 x)]\n",
    "\\\\]\n",
    "\n",
    "By the definition of $\\mathcal{N}$, that means:\n",
    "\n",
    "\\\\[\n",
    "\\text{Pr}_\\theta[Y = y | X = x]\n",
    "=\n",
    "    \\frac{1}{\\sqrt{2\\pi\\sigma'^2}}\n",
    "    \\exp \\left(\n",
    "        -\\frac{(y - (\\theta_0 + \\theta_1 x))^2}{2\\sigma'^2}\n",
    "    \\right)\n",
    "\\\\]\n",
    "\n",
    "Don't let this big formula scare you. Let's note a couple things. First, this probability is maximized by $y = \\theta_0 + \\theta_1 x$. That means that $\\theta_0 + \\theta_1 x$ is the *most probable* value of $y$ given $x$.\n",
    "\n",
    "Second, note that the probability that $y = \\theta_0 + \\theta_1 x + \\epsilon$ is always equal to the probability that $y = \\theta_0 + \\theta_1 x - \\epsilon$. That means that the probability distribution is *symmetric* arround $\\theta_0 + \\theta_1 x$. That means that $\\theta_0 + \\theta_1 x$ is both the median and mean value of $y$ given $x$.\n",
    "\n",
    "Using this probability distribution, we can calculate a probability for the *entire training dataset* $\\mathcal{D}$:\n",
    "\n",
    "\\\\[\n",
    "\\text{Pr}_\\theta[\\mathcal{D}]\n",
    "=\n",
    "\\prod_i \\text{Pr}_\\theta[Y = y_i | X = x_i]\n",
    "= \\prod_i \n",
    "    \\frac{1}{\\sqrt{2\\pi\\sigma'^2}}\n",
    "    \\exp \\left(\n",
    "        -\\frac{(y_i - (\\theta_0 + \\theta_1 x_i))^2}{2\\sigma'^2}\n",
    "    \\right)\n",
    "\\\\]\n",
    "\n",
    "The reason I write $\\text{Pr}_\\theta[\\mathcal{D}]$ is because this is the probability of the dataset being generated, presuming that the true model is $y = \\theta_0 + \\theta_1 + \\mathcal{N}(0, \\sigma')$. Different choices of $\\theta$ would lead to different probability distributions.\n",
    "\n",
    "We call $\\text{Pr}_\\theta[\\mathcal{D}]$ the *likelihood* of $\\theta$. It is not quite the same as the *probability* of $\\theta$. We will explore that difference later when we learn about Bayesian statistics.\n",
    "\n",
    "It is very common to want to consider the best $\\theta$ to be the one has the *maximum likelihood*. That is, we want to choose the $\\theta$ under which it would be most probable to generate our dataset $\\mathcal{D}$. This seems sensible: why would we want to prefer a $\\theta'$ if it does a worse job at predicting a dataset like the one we trained on?\n",
    "\n",
    "Whenever we have a big product of probabilities, it is common to work in the *negative log space*. This uses the following rule:\n",
    "\n",
    "\\\\[\n",
    "a \\times b \\times c\n",
    "= \\exp \\left( \\log \\left(a \\times b \\times c\\right)\\right)\n",
    "= \\exp \\left( \\log a + \\log b + \\log c\\right)\n",
    "\\\\]\n",
    "\n",
    "Likewise:\n",
    "\n",
    "\\\\[\n",
    "-\\log \\text{Pr}_\\theta[\\mathcal{D}]\n",
    "=\n",
    "- \\sum_i \\log \\text{Pr}_\\theta[Y = y_i | X = x_i]\n",
    "\\\\]\n",
    "\n",
    "Now, the $\\log$ function is a *monotonic transformation*. That means that if $a < b$, then $\\log a < \\log b$. Since I took the *negative* log, we have $a < b$ implies $-\\log b < -\\log a$. That means that maximizing the probability $\\text{Pr}_\\theta[\\mathcal{D}]$ is the same as minimizing $-\\log\\text{Pr}_\\theta[\\mathcal{D}]$.\n",
    "\n",
    "So let us now work this out further:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "- \\sum_i \\log \\text{Pr}_\\theta[Y = y_i | X = x_i]\n",
    "&=\n",
    "-\\sum_i \\log\n",
    "\\Big[\n",
    "    \\frac{1}{\\sqrt{2\\pi\\sigma'^2}}\n",
    "    \\exp \\left(\n",
    "        -\\frac{(y_i - (\\theta_0 + \\theta_1 x_i))^2}{2\\sigma'^2}\n",
    "    \\right)\n",
    "\\Big]\n",
    "\\\\\n",
    "&=\n",
    "-\\sum_i\n",
    "    \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma'^2}} \\right)\n",
    "    -\n",
    "    \\left(\n",
    "        \\frac{(y_i - (\\theta_0 + \\theta_1 x_i))^2}{2\\sigma'^2}\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "For the purposes of minimization, we cannot change $\\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma'^2}} \\right)$ by changing $\\theta$. Therefore it is a constant. Likewise $2\\sigma'^2$ is a constant. I will get rid of them, since they don't matter:\n",
    "\n",
    "\\\\[\n",
    "-\\sum_i -(y_i - (\\theta_0 + \\theta_1 x_i))^2 = \\sum_i (y_i - (\\theta_0 + \\theta_1 x_i))^2\n",
    "\\\\]\n",
    "\n",
    "And now I have gotten where I want to. This is the sum of squares error. What this shows is that minimizing the sum of squares error leads to the maximum likelihood estimate of $\\theta$. That is: minimizing the sum of squares error will pick the $\\theta$ which maximizes $\\text{Pr}_\\theta[\\mathcal{D}]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
