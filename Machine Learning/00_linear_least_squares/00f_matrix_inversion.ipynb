{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Inversion\n",
    "\n",
    "We saw that when the contours of a function are skewed, naively simultaneously minimizing the function wrt $\\hat\\theta_0, \\hat\\theta_1$ each independently will lead to zig-zagging. This is because a change to $\\hat\\theta_0$ changes the partial of the function wrt $\\hat\\theta_1$.\n",
    "\n",
    "Can avoid the zig-zagging? For quadratic functions we sure can! To start, let's recall how a change to $\\hat\\theta_0$ changes the first derivatives of $E$:\n",
    "\n",
    "\\\\[\n",
    "\\left( \\Delta \\frac{\\partial E}{\\partial \\theta_0}, \\Delta \\frac{\\partial E}{\\partial \\theta_1} \\right)\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_0^2}, \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1} \\right)\n",
    "\\cdot\n",
    "\\Delta \\hat\\theta_0\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_0^2} \\Delta \\hat\\theta_0,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1} \\Delta \\hat\\theta_0\n",
    "\\right)\n",
    "\\\\]\n",
    "\n",
    "This shows that the change in the each of the first derivatives is a linear function of the change to $\\hat\\theta_0$. Ideally the second, mixed term would be zero, but therein lies our problem.\n",
    "\n",
    "One important thing to note. Since the second partials are both *constant* in $\\hat\\theta_0$ since $E$ is a quadratic function (only has powers up to two), that means this equation is *exact*. We don't have to worry about the second partials changing on us as we change $\\hat\\theta_0$. In fact, the the second partials are also constant in $\\hat\\theta_1$ for the same reason!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the same equation, but with respect to changes in $\\hat\\theta_1$.\n",
    "\n",
    "\\\\[\n",
    "\\left( \\Delta \\frac{\\partial E}{\\partial \\theta_0}, \\Delta \\frac{\\partial E}{\\partial \\theta_1} \\right)\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0}, \\frac{\\partial^2 E}{\\partial \\theta_1^2} \\right)\n",
    "\\cdot\n",
    "\\Delta \\hat\\theta_1\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\Delta \\hat\\theta_1,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_1^2} \\Delta \\hat\\theta_1\n",
    "\\right)\n",
    "\\\\]\n",
    "\n",
    "Similar to last time, we have a linear function of $\\hat\\theta_1$, and these second partials are constant functions that don't change as $\\hat\\theta_0, \\hat\\theta_1$ change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then talk about how to calculate the change to the partials if you change *both* $\\hat\\theta_0$ and $\\hat\\theta_1$ simultaneously:\n",
    "\n",
    "\\\\[\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_0^2} \\Delta \\hat\\theta_0,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1} \\Delta \\hat\\theta_0\n",
    "\\right)\n",
    "+\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\Delta \\hat\\theta_1,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_1^2} \\Delta \\hat\\theta_1\n",
    "\\right)\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_0^2} \\Delta \\hat\\theta_0\n",
    "       + \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\Delta \\hat\\theta_1,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_1^2} \\Delta \\hat\\theta_1\n",
    "       + \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1} \\Delta \\hat\\theta_0\n",
    "\\right)\n",
    "\\\\]\n",
    "\n",
    "Okay. This formula defines a function:\n",
    "\n",
    "\\\\[\n",
    "f_H \\left((\\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1)\\right)\n",
    "=\n",
    "\\left( \\frac{\\partial^2 E}{\\partial \\theta_0^2} \\Delta \\hat\\theta_0\n",
    "       + \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\Delta \\hat\\theta_1,\n",
    "       \\frac{\\partial^2 E}{\\partial \\theta_1^2} \\Delta \\hat\\theta_1\n",
    "       + \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1} \\Delta \\hat\\theta_0\n",
    "\\right)\n",
    "\\\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function $f_H$ is a *linear transformation* from $\\mathbb{R}^2 \\to \\mathbb{R}^2$. What I mean is that the input is a multi-dimensional quantity: it consists of two real numbers that represent changes to $\\hat\\theta_0, \\hat\\theta_1$. The output of $f_H$ is also multidimensional: it consists of two real numbers that represent the corresponding changes to $\\frac{\\partial E}{\\partial \\theta_0}, \\frac{\\partial E}{\\partial \\theta_1}$.\n",
    "\n",
    "Any linear transformation can be written as a matrix. Let me show you how:\n",
    "\n",
    "\\\\[\n",
    "f_H \\left((\\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1)\\right)\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0^2}\n",
    "& \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1}\n",
    "& \\frac{\\partial^2 E}{\\partial \\theta_1^2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Delta \\hat\\theta_0 \\\\\n",
    "\\Delta \\hat\\theta_1\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "How do you multiply a matrix and a vector? Something about rows and columns, right? What if I told you your whole life was a lie:\n",
    "\n",
    "\\\\[\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0^2}\n",
    "& \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1}\n",
    "& \\frac{\\partial^2 E}{\\partial \\theta_1^2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\Delta \\hat\\theta_0 \\\\\n",
    "\\Delta \\hat\\theta_1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\Delta \\hat\\theta_0\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0^2} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\Delta \\hat\\theta_1\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0} \\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\theta_1^2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\Delta \\hat\\theta_0 \\frac{\\partial^2 E}{\\partial \\theta_0^2} \n",
    "+ \\Delta \\hat\\theta_1 \\frac{\\partial^2 E}{\\partial \\theta_1 \\partial \\theta_0}\n",
    "\\\\\n",
    "\\Delta \\hat\\theta_0 \\frac{\\partial^2 E}{\\partial \\theta_0 \\partial \\theta_1}\n",
    "+ \\Delta \\hat\\theta_1 \\frac{\\partial^2 E}{\\partial \\theta_1^2}\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "What wizardry did I do? Instead of taking the *dot product* of the rows of the matrix with the column vector, I took a *weighted sum* of the columns of the matrix, weighting by the entries of the column vector. You can verify that both approaches are the same.\n",
    "\n",
    "Why do I like my weighted sum of columns way? I like it because it shows how the final vector output is the sum of two independent components: the change from $\\Delta \\hat\\theta_0$ and the change from $\\Delta \\hat\\theta_1$. The columns represent how much change there is per unit of change to $\\Delta \\hat\\theta_0, \\Delta\\hat\\theta_1$.\n",
    "\n",
    "A matrix of second partial derivatives like this is called the *Hessian* matrix. Because we are dealing with a quadratic function $E$, the Hessian matrix is constant everywhere. Thus the change in $(\\frac{\\partial E}{\\partial \\hat\\theta_0}, \\frac{\\partial E}{\\partial \\hat\\theta_1})$ caused by a change $(\\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1)$ is always the same, no matter the original or changed values of $(\\hat\\theta_0, \\hat\\theta_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the transformation:\n",
    "\n",
    "\\\\[\n",
    "\\left( \\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1 \\right)\n",
    "\\mapsto\n",
    "\\left( \\Delta \\frac{\\partial E}{\\partial \\hat\\theta_0}, \\Delta \\frac{\\partial E}{\\partial \\hat\\theta_1} \\right)\n",
    "\\\\]\n",
    "\n",
    "Remember our goal is to find points where both partial derivatives are zero. Let me drop some notation on you. Recall that the *gradient* is:\n",
    "\n",
    "\\\\[\n",
    "\\nabla E\\left( (\\hat\\theta_0, \\hat\\theta_1) \\right) = \\left(\n",
    "    \\frac{\\partial E}{\\partial \\hat\\theta_0} \\left( (\\hat\\theta_0, \\hat\\theta_1) \\right),\n",
    "    \\frac{\\partial E}{\\partial \\hat\\theta_1} \\left( (\\hat\\theta_0, \\hat\\theta_1) \\right)\n",
    "\\right)\n",
    "\\\\]\n",
    "\n",
    "The gradient, as you can see, is just the vector of first partial derivatives. We want it to be zero at each coordinate. Using the gradient, let me rewrite the mapping defined by the Hessian $H$ above:\n",
    "\n",
    "\\\\[\n",
    "\\left( \\Delta \\hat\\theta_0, \\Delta \\hat\\theta_1 \\right)\n",
    "\\mapsto\n",
    "\\Delta \\left( \\nabla E \\right)\n",
    "\\\\]\n",
    "\n",
    "So let's say we are at a point $(\\hat\\theta_0, \\hat\\theta_1)$, so the gradient is $\\nabla E\\left( (\\hat\\theta_0, \\hat\\theta_1) \\right)$. Then what we want to do is *change* the gradient by $-\\nabla E\\left( (\\hat\\theta_0, \\hat\\theta_1) \\right)$. Basically, we want to run $f_H$ *in reverse*.\n",
    "\n",
    "The function that *reverses* is called the *inverse* function. We want the inverse of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a simple example. If\n",
    "\n",
    "\\\\[\n",
    "H\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "3 & 0 \\\\\n",
    "0 & 5\n",
    "\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "This is a simple matrix representing a Hessian where the mixed partials are zero. We saw previously that this corresponds to a parabolic surface with axes parallel to coordinate axes. Let's see:\n",
    "\n",
    "\\\\[\n",
    "H\n",
    "\\begin{bmatrix}a \\\\ b\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}3a \\\\ 0a\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}0b \\\\ 5b\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}3a \\\\ 5b\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "A matrix like our example $H$ is called a *diagonal matrix*. A diagonal matrix simply *stretches* each individual coordinate. To go back, we need to *undo* the stretching. We want a matrix $H^{-1}$ such that:\n",
    "\n",
    "\\\\[\n",
    "H^{-1}\n",
    "\\begin{bmatrix}3a \\\\ 5b\\end{bmatrix}\n",
    "\\mapsto\n",
    "\\begin{bmatrix}a \\\\ b\\end{bmatrix}\n",
    "\\\\]\n",
    "\n",
    "Well, if stretching got us into this mess, let's use shrinking to get us out. Let:\n",
    "\n",
    "\\\\[\n",
    "H^{-1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{3} & 0 \\\\\n",
    "0 & \\frac{1}{5}\n",
    "\\end{bmatrix} \\\\\n",
    "H^{-1}\n",
    "\\begin{bmatrix}3a \\\\ 5b\\end{bmatrix}\n",
    "\\mapsto\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{3} & 0 \\\\\n",
    "0 & \\frac{1}{5}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}3a \\\\ 5b\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}a \\\\ b\\end{bmatrix}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we know that applying a linear transformation and undoing it should keep things the same. Watch this:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "H^{-1}H\n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "        3 & 0 \\\\\n",
    "        0 & 5\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\frac{1}{3} & 0 \\\\\n",
    "        0 & \\frac{1}{5}\n",
    "    \\end{bmatrix}\\\\\n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "        3 & 0 \\\\\n",
    "        0 & 5\n",
    "        \\end{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "        \\frac{1}{3} \\\\ 0\n",
    "        \\end{bmatrix}\n",
    "    &\n",
    "        \\begin{bmatrix}\n",
    "        3 & 0 \\\\\n",
    "        0 & 5\n",
    "        \\end{bmatrix}\n",
    "        \\begin{bmatrix}\n",
    "        0 \\\\ \\frac{1}{5}\n",
    "        \\end{bmatrix}\n",
    "    \\end{bmatrix} \\\\\n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "        \\left(\n",
    "            \\begin{bmatrix}\n",
    "            3 \\cdot \\frac{1}{3} \\\\\n",
    "            0 \\cdot \\frac{1}{3}\n",
    "            \\end{bmatrix}\n",
    "            +\n",
    "            \\begin{bmatrix}\n",
    "            0 \\cdot 0 \\\\\n",
    "            5 \\cdot 0\n",
    "            \\end{bmatrix}\n",
    "        \\right)\n",
    "    &\n",
    "        \\left(\n",
    "            \\begin{bmatrix}\n",
    "            3 \\cdot 0 \\\\\n",
    "            0 \\cdot 0\n",
    "            \\end{bmatrix}\n",
    "            +\n",
    "            \\begin{bmatrix}\n",
    "            0 \\cdot \\frac{1}{5} \\\\\n",
    "            5 \\frac{1}{5}\n",
    "            \\end{bmatrix}\n",
    "        \\right)\n",
    "    \\end{bmatrix}\\\\\n",
    "&=\n",
    "    \\begin{bmatrix}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1\n",
    "    \\end{bmatrix}\n",
    "\\end{align}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did I choose to multiply this way? Well, remember that the first column of $H^{-1}H$ is where the vector $(1, 0)$ should be mapped to. How do we calculate that? Well, we first ask: where does $(1, 0)$ go when we apply $H$ to it? Answer: the first column of $H$.\n",
    "\n",
    "The second step of $H^{-1}H$ is to apply $H^{-1}$. So we apply $H^{-1}$ to the first column of $H$. By doing so we have calculated where $(1, 0)$ should go under the combined transformation $H^{-1}H$.\n",
    "\n",
    "Of course, the product of a matrix and its inverse is a diagonal matrix with ones along the diagonal. This says: keep vectors the same. We typically write a matrix like this as $I$, called the *identity matrix*, which corresponds to the *identity transformation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverting non-diagonal matrices isn't nearly as easy as inverting diagonal matrices (which is trivial). A common way to do this is to use [Gauss-Jordan Elimination](https://en.wikipedia.org/wiki/Gaussian_elimination). However, I won't show how to do this.\n",
    "\n",
    "There are two primary facts to know about matrix inversion. First, Gauss-Jordan elimination, if performed naively, can be *numerically unstable*. Basically: there can be a lot of \"roundoff\" error in the calculation of the inverse. This is typically not a problem if you are using a library like Numpy to invert matrices.\n",
    "\n",
    "The second fact to know is that matrix inversion is $O(k^3)$, where $k$ is the number of dimensions of the input/output space. For large matrices (with hundreds or thousands or tens of thousands of columns), matrix inversion is far too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, this is a perfectly acceptable method when there are two parameters $\\hat\\theta_0, \\hat\\theta_1$. Let's start from an initial point $(1.5, 0)$. We'll then calculate $\\nabla E((1.5, 0))$. We want to *undo* this gradient. That is: we want to change the gradient by $-\\nabla E((1.5, 0))$.\n",
    "\n",
    "How much do we need to move to do this? The answer is found by applying the matrix $H^{-1}$ like so:\n",
    "\n",
    "\\\\[\n",
    "\\begin{align}\n",
    "(\\hat\\theta_0, \\hat\\theta_1) &= (1.5, 0) + H^{-1} \\Big(-\\nabla E\\big((1.5, 0)\\big) \\Big) \\\\\n",
    "&= (1.5, 0) - H^{-1} \\Big(\\nabla E\\big((1.5, 0)\\big) \\Big)\n",
    "\\end{align}\n",
    "\\\\]\n",
    "\n",
    "Notice how that looks really similar to $x - f'(x) / f''(x)$? We've finally found the multidimensional version of Newton's Method!\n",
    "\n",
    "Okay, let's do it with code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial | theta0 = 1.50 | theta1 = 0.00 | gradient = [ 300.          102.86542613]\n",
      "Calculated | theta0 = -0.99 | theta1 = 0.99 | gradient = [  6.75015599e-14  -1.50990331e-14]\n"
     ]
    }
   ],
   "source": [
    "from examples.exact_quadratic_optimization_example import ExactQuadraticOptimizationExample\n",
    "\n",
    "ExactQuadraticOptimizationExample.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! A single step! As mentioned, this will be too slow when the input $x$ is a vector with thousands of dimensions. Another problem is that we will eventually explore problems where the *error surface* is not a quadratic bowl. In that case, this matrix inversion method won't be guaranteed to find a minimum in a single step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
